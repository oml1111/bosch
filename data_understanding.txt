For this project, the data gathering step is trivial and fixed, we just have to download the data from the Kaggle contest website and thats all we can use.

The data comes in standard CSV files. The data fields are split into 3 files, one for numeric fields, one for date fields and one for categorical fields. The files are large, 2.1GB for numeric, 2.6GB for categorical and 2.8GB for date training data, and similar size files for the testing data. As for data itself, there are ~2100 categorical, ~950 numeric and ~1100 date fields and there are about 1,200,000 entries. One interesting thing is that the data is fairly sparse (categorical fields especially so), this could help save some resources when eventually processing the data. The fields themselves don't have meaningful names, so it will be almost impossible to do human level analysis.

The numeric data itself consists of floating point numbers between -1 and 1. The average standard deviation for a field is 0.1, so data is close to zero, but still reasonably spread out. The dates are floating point numbers between 0.41 and 1718.46, so they have already been preprocessed somewhat. Finally the categorical data is a bunch of strings of the form "", "T1", "T2", ..., "T-21474872", but mostly it's "T1" or nothing. The data is preprocessed, so there is probably not much preprocessing and cleaning to do.

Data quality verification is very simple. In the leaderboard there are a lot of teams who have successfully submitted their solutions and gotten a score. Everyone presumably gets the same data so that is a good evidence that the data is useful. A manual look at the data itself and some processing scripts haven't come up with any problems.
